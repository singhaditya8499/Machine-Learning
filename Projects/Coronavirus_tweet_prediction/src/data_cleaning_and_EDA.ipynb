{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "e95b7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "eb1bbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data = pd.read_csv(\"../data/Corona_NLP_train.csv\", encoding='ISO-8859-1')\n",
    "# uncleaned_data = pd.read_csv(\"../data/Corona_NLP_train.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "9bacbe3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 6)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "7b706b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...   Neutral  \n",
       "1  advice Talk to your neighbours family to excha...  Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...  Positive  "
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "20417663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet',\n",
       "       'Sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b24eb714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserName          int64\n",
       "ScreenName        int64\n",
       "Location         object\n",
       "TweetAt          object\n",
       "OriginalTweet    object\n",
       "Sentiment        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "cd174586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserName            0\n",
       "ScreenName          0\n",
       "Location         8590\n",
       "TweetAt             0\n",
       "OriginalTweet       0\n",
       "Sentiment           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "97f2cd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['London', 'UK', 'Vagabonds', nan, 'Ãƒ\\x9cT: 36.319708,-82.363649',\n",
       "       '35.926541,-78.753267', 'Austria', 'Atlanta, GA USA',\n",
       "       'BHAVNAGAR,GUJRAT', 'Makati, Manila', 'Pitt Meadows, BC, Canada ',\n",
       "       'Horningsea', 'Chicago, IL', 'Houston, Texas', 'Saudi Arabia',\n",
       "       'Ontario, Canada', 'North America', 'Denver, CO',\n",
       "       'southampton soxx xxx', 'Global'], dtype=object)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.Location.unique()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b475f21",
   "metadata": {},
   "source": [
    "Based on initial observation, we can see that `Location` is a very abstract concept here and it can see multiple values. We are going to drop it as we dont see any significance to its use in our future model of text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "f69088a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data.drop(columns=['Location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "d55e89e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-03-2020    3448\n",
       "19-03-2020    3215\n",
       "25-03-2020    2979\n",
       "18-03-2020    2742\n",
       "21-03-2020    2653\n",
       "22-03-2020    2114\n",
       "23-03-2020    2062\n",
       "17-03-2020    1977\n",
       "08-04-2020    1881\n",
       "07-04-2020    1843\n",
       "Name: TweetAt, dtype: int64"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.TweetAt.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a729cca",
   "metadata": {},
   "source": [
    "`Tweet at` date might have some significance, but in the grander scheme of things it might not hold any significant value. We can safely drop it. In any case, the tweets are collected for a very short duration of around 20 days. If we look back, the information regarding COVID constantly kept on evolving. All these observations can be taken into consideration and `Tweet at` date can be safely dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "69c88925",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data.drop(columns=['TweetAt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "84378215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41157"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uncleaned_data.UserName.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb89e3",
   "metadata": {},
   "source": [
    "We can see that the total unique `UserName` in the dataset is equal to the total data points. Since, we dont actually get any information from the `UserName`, we can safely drop it to simplify the dimensions of the dataset.\n",
    "\n",
    "Same is the case with `ScreenName`. So we will drop it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "d5d32026",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data.drop(columns=['UserName', 'ScreenName'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "c51183c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OriginalTweet', 'Sentiment'], dtype='object')"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "176294d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral', 'Positive']"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(uncleaned_data.Sentiment.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd4bc2",
   "metadata": {},
   "source": [
    "The unique value in Sentiment is what we expected, it is clean and there is no need to clean it further. In the later sections we can convert it One-hot encoding or some other types based on the model we choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "23a20a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"All month there hasn't been crowding in the supermarkets or restaurants, however reducing all the hours and closing the malls means everyone is now using the same entrance and dependent on a single supermarket. #manila #lockdown #covid2019 #Philippines https://t.co/HxWs9LAnF9\",\n",
       " 'Neutral')"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.OriginalTweet[10], uncleaned_data.Sentiment[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7c64c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = ' '.join(uncleaned_data.OriginalTweet.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6726abe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x80', '\\x84', '\\x85', '\\x87', '\\x89', '\\x91', '\\x92', '\\x93', '\\x94', '\\x95', '\\x96', '\\x97', '\\x98', '\\x99', '\\x9a', '\\x9e', '\\x9f', '\\xa0', 'Â¡', 'Â¢', 'Â£', 'Â¤', 'Â¥', 'Â¦', 'Â§', 'Â¨', 'Â©', 'Â«', '\\xad', 'Â®', 'Â¯', 'Â°', 'Â±', 'Â²', 'Â³', 'Â´', 'Â¶', 'Â·', 'Â¸', 'Â¹', 'Âº', 'Â»', 'Â¼', 'Â½', 'Ã‚', 'Ãƒ']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(list(set(allText))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fcb491",
   "metadata": {},
   "source": [
    "As we can see there are too unwanted characters in the tweet. We are going to remove them and focus on the content of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c84b6829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove digits\n",
    "    text = re.sub(r'\\d+',' ', text)\n",
    "    \n",
    "    # remove urls\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    \n",
    "    # remove html tags\n",
    "    text = re.sub(r'<.*?>',' ', text)\n",
    "    \n",
    "    # remove digits\n",
    "    text = re.sub(r'\\d+',' ', text)\n",
    "    \n",
    "    # remove hashtags\n",
    "    text = re.sub(r'#\\w+',' ', text)\n",
    "    \n",
    "    # remove mentions\n",
    "    text = re.sub(r'@\\w+',' ', text)\n",
    "    \n",
    "    # Remove any extra spaces\n",
    "    text = re.sub(' +', ' ', text.strip())\n",
    "    \n",
    "    # Remove everything that is not space and alphabets\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    # Converting to lower case\n",
    "    cleaned_text = text.lower()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "e34778c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data.OriginalTweet = uncleaned_data.OriginalTweet.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9f23ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     words = text.split()\n",
    "#     filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "#     return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "3de75dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncleaned_data.OriginalTweet = uncleaned_data.OriginalTweet.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "6798bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_words_with_less_than_2_characters(text):\n",
    "#     list_of_words = text.split(' ')\n",
    "#     list_of_words = [x for x in list_of_words if len(x) > 2]\n",
    "#     return ' '.join(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "119fecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncleaned_data.OriginalTweet = uncleaned_data.OriginalTweet.apply(remove_words_with_less_than_2_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "9d53e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# def stemming(text):\n",
    "#     # Tokenize the text\n",
    "#     words = word_tokenize(text)\n",
    "\n",
    "#     # Create a Porter Stemmer\n",
    "#     porter = PorterStemmer()\n",
    "\n",
    "#     # Perform stemming\n",
    "#     stemmed_words = [porter.stem(word) for word in words]\n",
    "#     return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9b479eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncleaned_data.OriginalTweet = uncleaned_data.OriginalTweet.apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ea71a665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/adityasingh/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# we are just checking whether the word exist in the dictionary or not\n",
    "\n",
    "import nltk\n",
    "# Download the words dataset from NLTK (if not already downloaded)\n",
    "nltk.download('words')\n",
    "valid_words = set(nltk.corpus.words.words())\n",
    "\n",
    "def allow_valid_words(text):\n",
    "    words = [x for x in text.split(' ') if x in valid_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "aca0b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data.OriginalTweet = uncleaned_data.OriginalTweet.apply(allow_valid_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "6a7aa9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words are: 13026\n"
     ]
    }
   ],
   "source": [
    "tmp = list(set(' '.join(uncleaned_data.OriginalTweet.tolist()).split(' ')))\n",
    "print(f\"Total unique words are: {len(tmp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd02a0",
   "metadata": {},
   "source": [
    "We have removed any unwanted characters using the `clean_text` function above. We kept only the characters between a-z. Now that we have cleaned the text, we can save it for later use in different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c3888625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OriginalTweet    0\n",
       "Sentiment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "71e25cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data.to_csv('../data/cleaned_data.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "6786e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.read_csv(\"../data/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf0ce9",
   "metadata": {},
   "source": [
    "We can see that after saving there are few entries that became Nan, that is because those entries were emply when they were saved. We can filter them out and rewrite over the existing saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "3536be61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data.OriginalTweet.iloc[[i for i, x in enumerate(cleaned_data.OriginalTweet.isna()) if x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "3580b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "f7e2aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv(\"../data/cleaned_data.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "8ae3f92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fan pa and and</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice talk to your family to exchange phone c...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to give elderly disabled shopping amid covid o...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet Sentiment\n",
       "0                                     fan pa and and   Neutral\n",
       "1  advice talk to your family to exchange phone c...  Positive\n",
       "2  to give elderly disabled shopping amid covid o...  Positive"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data = pd.read_csv(\"../data/cleaned_data.csv\")\n",
    "cleaned_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "a03a86bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = cleaned_data.OriginalTweet.tolist()\n",
    "smallTweets = [x for x in texts if len(x.split())<15]\n",
    "textLen = [len(x) for x in texts]\n",
    "sorted(textLen)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "588881a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37701"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moreThan8Words = [x for x in texts if len(x.split(' ')) > 7]\n",
    "moreThan8Words = sorted(moreThan8Words, key=lambda x: len(x.split(' ')))\n",
    "len(moreThan8Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281058f",
   "metadata": {},
   "source": [
    "Based on a small experiment, we can see that there are around 39346 samples with total words of more than 7. We are going to go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "05b9a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(x):\n",
    "    return len(x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "8a513162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41104, 2)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# super_cleaned_data = cleaned_data[cleaned_data['OriginalTweet'].apply(count_words) > 7]\n",
    "super_cleaned_data = cleaned_data\n",
    "super_cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "eac2866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_cleaned_data.to_csv(\"../data/super_cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "1907aaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words are: 13025\n"
     ]
    }
   ],
   "source": [
    "tmp = list(set(' '.join(super_cleaned_data.OriginalTweet.tolist()).split(' ')))\n",
    "print(f\"Total unique words are: {len(tmp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "1c026276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive              11409\n",
       "Negative               9916\n",
       "Neutral                7675\n",
       "Extremely Positive     6623\n",
       "Extremely Negative     5481\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_cleaned_data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b55fd3",
   "metadata": {},
   "source": [
    "**Now we have the cleaned data and we can go ahead and start working on building model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc7352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
