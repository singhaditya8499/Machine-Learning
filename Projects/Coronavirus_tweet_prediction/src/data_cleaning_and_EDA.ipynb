{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95b7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1bbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data = pd.read_csv(\"../data/Corona_NLP_train.csv\", encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bacbe3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b706b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...   Neutral  \n",
       "1  advice Talk to your neighbours family to excha...  Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...  Positive  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20417663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet',\n",
       "       'Sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b24eb714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserName          int64\n",
       "ScreenName        int64\n",
       "Location         object\n",
       "TweetAt          object\n",
       "OriginalTweet    object\n",
       "Sentiment        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd174586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserName            0\n",
       "ScreenName          0\n",
       "Location         8590\n",
       "TweetAt             0\n",
       "OriginalTweet       0\n",
       "Sentiment           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97f2cd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['London', 'UK', 'Vagabonds', nan, 'Ãƒ\\x9cT: 36.319708,-82.363649',\n",
       "       '35.926541,-78.753267', 'Austria', 'Atlanta, GA USA',\n",
       "       'BHAVNAGAR,GUJRAT', 'Makati, Manila', 'Pitt Meadows, BC, Canada ',\n",
       "       'Horningsea', 'Chicago, IL', 'Houston, Texas', 'Saudi Arabia',\n",
       "       'Ontario, Canada', 'North America', 'Denver, CO',\n",
       "       'southampton soxx xxx', 'Global'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.Location.unique()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b475f21",
   "metadata": {},
   "source": [
    "Based on initial observation, we can see that `Location` is a very abstract concept here and it can see multiple values. We are going to drop it as we dont see any significance to its use in our future model of text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f69088a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data.drop(columns=['Location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d55e89e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-03-2020    3448\n",
       "19-03-2020    3215\n",
       "25-03-2020    2979\n",
       "18-03-2020    2742\n",
       "21-03-2020    2653\n",
       "22-03-2020    2114\n",
       "23-03-2020    2062\n",
       "17-03-2020    1977\n",
       "08-04-2020    1881\n",
       "07-04-2020    1843\n",
       "Name: TweetAt, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.TweetAt.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a729cca",
   "metadata": {},
   "source": [
    "`Tweet at` date might have some significance, but in the grander scheme of things it might not hold any significant value. We can safely drop it. In any case, the tweets are collected for a very short duration of around 20 days. If we look back, the information regarding COVID constantly kept on evolving. All these observations can be taken into consideration and `Tweet at` date can be safely dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69c88925",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data.drop(columns=['TweetAt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84378215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41157"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uncleaned_data.UserName.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb89e3",
   "metadata": {},
   "source": [
    "We can see that the total unique `UserName` in the dataset is equal to the total data points. Since, we dont actually get any information from the `UserName`, we can safely drop it to simplify the dimensions of the dataset.\n",
    "\n",
    "Same is the case with `ScreenName`. So we will drop it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5d32026",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data.drop(columns=['UserName', 'ScreenName'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c51183c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OriginalTweet', 'Sentiment'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "176294d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral', 'Positive']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(uncleaned_data.Sentiment.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd4bc2",
   "metadata": {},
   "source": [
    "The unique value in Sentiment is what we expected, it is clean and there is no need to clean it further. In the later sections we can convert it One-hot encoding or some other types based on the model we choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23a20a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"All month there hasn't been crowding in the supermarkets or restaurants, however reducing all the hours and closing the malls means everyone is now using the same entrance and dependent on a single supermarket. #manila #lockdown #covid2019 #Philippines https://t.co/HxWs9LAnF9\",\n",
       " 'Neutral')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.OriginalTweet[10], uncleaned_data.Sentiment[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c64c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = ' '.join(uncleaned_data.OriginalTweet.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6726abe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x80', '\\x84', '\\x85', '\\x87', '\\x89', '\\x91', '\\x92', '\\x93', '\\x94', '\\x95', '\\x96', '\\x97', '\\x98', '\\x99', '\\x9a', '\\x9e', '\\x9f', '\\xa0', 'Â¡', 'Â¢', 'Â£', 'Â¤', 'Â¥', 'Â¦', 'Â§', 'Â¨', 'Â©', 'Â«', '\\xad', 'Â®', 'Â¯', 'Â°', 'Â±', 'Â²', 'Â³', 'Â´', 'Â¶', 'Â·', 'Â¸', 'Â¹', 'Âº', 'Â»', 'Â¼', 'Â½', 'Ã‚', 'Ãƒ']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(list(set(allText))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fcb491",
   "metadata": {},
   "source": [
    "As we can see there are too unwanted characters in the tweet. We are going to remove them and focus on the content of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c84b6829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text_without_hashtags = re.sub(r'#\\S+', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text_without_urls = re.sub(r'http\\S+|www\\S+|https\\S+', '', text_without_hashtags)\n",
    "    \n",
    "    # Remove mentions\n",
    "    text_without_mentions = re.sub(r'@\\S+', '', text_without_urls)\n",
    "    \n",
    "    # Remove any extra spaces\n",
    "    text_without_extra_spaces = ' '.join(text_without_mentions.split())\n",
    "    # Converting to lower case\n",
    "    lower_case_text = text_without_extra_spaces.lower()\n",
    "    cleaned_text = re.sub(r'[^a-z0-9 ]', '', lower_case_text.lower())\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e34778c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data.OriginalTweet = uncleaned_data.OriginalTweet.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf68555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfd90ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data.OriginalTweet = uncleaned_data.OriginalTweet.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd02a0",
   "metadata": {},
   "source": [
    "We have removed any unwanted characters using the `clean_text` function above. We kept all the characters between 0-9 and a-z. Now that we have cleaned the text, we can save it for later use in different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3888625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OriginalTweet    0\n",
       "Sentiment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71e25cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data.to_csv('../data/cleaned_data.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6786e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.read_csv(\"../data/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf0ce9",
   "metadata": {},
   "source": [
    "We can see that after saving there are few entries that became Nan, that is because those entries were emply when they were saved. We can filter them out and rewrite over the existing saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3536be61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data.OriginalTweet.iloc[[i for i, x in enumerate(cleaned_data.OriginalTweet.isna()) if x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3580b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7e2aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv(\"../data/cleaned_data.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ae3f92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advice talk neighbours family exchange phone n...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coronavirus australia woolworths give elderly ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food stock one empty please dont panic enough ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet Sentiment\n",
       "0  advice talk neighbours family exchange phone n...  Positive\n",
       "1  coronavirus australia woolworths give elderly ...  Positive\n",
       "2  food stock one empty please dont panic enough ...  Positive"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data = pd.read_csv(\"../data/cleaned_data.csv\")\n",
    "cleaned_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a03a86bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 2, 2, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = cleaned_data.OriginalTweet.tolist()\n",
    "smallTweets = [x for x in texts if len(x.split())<15]\n",
    "textLen = [len(x) for x in texts]\n",
    "sorted(textLen)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "588881a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35705"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moreThan8Words = [x for x in texts if len(x.split(' ')) > 7]\n",
    "moreThan8Words = sorted(moreThan8Words, key=lambda x: len(x.split(' ')))\n",
    "len(moreThan8Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281058f",
   "metadata": {},
   "source": [
    "Based on a small experiment, we can see that there are around 39346 samples with total words of more than 7. We are going to go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05b9a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(x):\n",
    "    return len(x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a513162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35705, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_cleaned_data = cleaned_data[cleaned_data['OriginalTweet'].apply(count_words) > 7]\n",
    "super_cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eac2866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_cleaned_data.to_csv(\"../data/super_cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c026276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive              10169\n",
       "Negative               8877\n",
       "Extremely Positive     6338\n",
       "Extremely Negative     5235\n",
       "Neutral                5086\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_cleaned_data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b55fd3",
   "metadata": {},
   "source": [
    "**Now we have the cleaned data and we can go ahead and start working on building model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc7352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
