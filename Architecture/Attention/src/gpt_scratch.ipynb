{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89288632",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ea23d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/songs.txt', 'r', encoding='utf-8') as f:\n",
    "    plain_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40632fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612105"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(plain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc78b1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song Name: Tim McGraw\n",
      "He said the way my blue eyes shined\n",
      "Put those Georgia stars to shame that night\n",
      "I said Thats a lie\n",
      "Just a boy in a Chevy truck\n",
      "That had a tendency of gettin stuck\n",
      "On back roads at night\n",
      "And I was right there beside him all summer long\n",
      "And then the time we woke up to find that summer gone\n",
      "\n",
      "But when you think Tim McGraw\n",
      "I hope you think my favorite song\n",
      "The one we danced to all night long\n",
      "The moon like a spotlight on the lake\n",
      "When you think happiness\n",
      "I hope you think that lit\n"
     ]
    }
   ],
   "source": [
    "print(plain_data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65278cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !&'(),-./0123456789:?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz®éíïó\n"
     ]
    }
   ],
   "source": [
    "# seeing all the unique characters in the data to perform the cleaning\n",
    "print(''.join(sorted(list(set(plain_data)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a02fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the unwanted characters with the most probable ones\n",
    "\n",
    "# ® -> ''\n",
    "# é -> e\n",
    "# ó -> o\n",
    "# í -> i\n",
    "# ï -> i\n",
    "\n",
    "plain_data = plain_data.replace('®', '')\n",
    "plain_data = plain_data.replace('é', 'e')\n",
    "plain_data = plain_data.replace('ó', 'o')\n",
    "plain_data = plain_data.replace('í', 'i')\n",
    "plain_data = plain_data.replace('ï', 'i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6daa406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !&'(),-./0123456789:?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(plain_data)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1878ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic tokenizer\n",
    "# converting each character to its index in the unique characters array\n",
    "# Google uses SentencePiece to tokenize\n",
    "# OpenAI uses tiktoken to tokenize\n",
    "\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for i,s in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[i] for i in s]\n",
    "decode = lambda s: ''.join([itos[i] for i in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c42573a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data is:  [56, 53, 60, 60, 63, 1, 71, 63, 66, 60, 52]\n"
     ]
    }
   ],
   "source": [
    "encoded_data = encode('hello world')\n",
    "print(\"Encoded data is: \", encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eaf4bd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded data is:  hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoded data is: \", decode(encoded_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14a16b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23b38076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([612104]) torch.int64\n",
      "tensor([41, 63, 62, 55,  1, 36, 49, 61, 53, 21,  1, 42, 57, 61,  1, 35, 51, 29,\n",
      "        66, 49, 71,  0, 30, 53,  1, 67, 49, 57, 52,  1, 68, 56, 53,  1, 71, 49,\n",
      "        73,  1, 61, 73,  1, 50, 60, 69, 53,  1, 53, 73, 53, 67,  1, 67, 56, 57,\n",
      "        62, 53, 52,  0, 38, 69, 68,  1, 68, 56, 63, 67, 53,  1, 29, 53, 63, 66,\n",
      "        55, 57, 49,  1, 67, 68, 49, 66, 67,  1, 68, 63,  1, 67, 56, 49, 61, 53,\n",
      "         1, 68, 56, 49, 68,  1, 62, 57, 55, 56, 68,  0, 31,  1, 67, 49, 57, 52,\n",
      "         1, 42, 56, 49, 68, 67,  1, 49,  1, 60, 57, 53,  0, 32, 69, 67, 68,  1,\n",
      "        49,  1, 50, 63, 73,  1, 57, 62,  1, 49,  1, 25, 56, 53, 70, 73,  1, 68,\n",
      "        66, 69, 51, 59,  0, 42, 56, 49, 68,  1, 56, 49, 52,  1, 49,  1, 68, 53,\n",
      "        62, 52, 53, 62, 51, 73,  1, 63, 54,  1, 55, 53, 68, 68, 57, 62,  1, 67,\n",
      "        68, 69, 51, 59,  0, 37, 62,  1, 50, 49, 51, 59,  1, 66, 63, 49, 52, 67,\n",
      "         1, 49, 68,  1, 62, 57, 55, 56, 68,  0, 23, 62, 52,  1, 31,  1, 71, 49,\n",
      "        67,  1, 66, 57, 55, 56, 68,  1, 68, 56, 53, 66, 53,  1, 50, 53, 67, 57,\n",
      "        52, 53,  1, 56, 57, 61,  1, 49, 60, 60,  1, 67, 69, 61, 61, 53, 66,  1,\n",
      "        60, 63, 62, 55,  0, 23, 62, 52,  1, 68, 56, 53, 62,  1, 68, 56, 53,  1,\n",
      "        68, 57, 61, 53,  1, 71, 53,  1, 71, 63, 59, 53,  1, 69, 64,  1, 68, 63,\n",
      "         1, 54, 57, 62, 52,  1, 68, 56, 49, 68,  1, 67, 69, 61, 61, 53, 66,  1,\n",
      "        55, 63, 62, 53,  0,  0, 24, 69, 68,  1, 71, 56, 53, 62,  1, 73, 63, 69,\n",
      "         1, 68, 56, 57, 62, 59,  1, 42, 57, 61,  1, 35, 51, 29, 66, 49, 71,  0,\n",
      "        31,  1, 56, 63, 64, 53,  1, 73, 63, 69,  1, 68, 56, 57, 62, 59,  1, 61,\n",
      "        73,  1, 54, 49, 70, 63, 66, 57, 68, 53,  1, 67, 63, 62, 55,  0, 42, 56,\n",
      "        53,  1, 63, 62, 53,  1, 71, 53,  1, 52, 49, 62, 51, 53, 52,  1, 68, 63,\n",
      "         1, 49, 60, 60,  1, 62, 57, 55, 56, 68,  1, 60, 63, 62, 55,  0, 42, 56,\n",
      "        53,  1, 61, 63, 63, 62,  1, 60, 57, 59, 53,  1, 49,  1, 67, 64, 63, 68,\n",
      "        60, 57, 55, 56, 68,  1, 63, 62,  1, 68, 56, 53,  1, 60, 49, 59, 53,  0,\n",
      "        45, 56, 53, 62,  1, 73, 63, 69,  1, 68, 56, 57, 62, 59,  1, 56, 49, 64,\n",
      "        64, 57, 62, 53, 67, 67,  0, 31,  1, 56, 63, 64, 53,  1, 73, 63, 69,  1,\n",
      "        68, 56, 57, 62, 59,  1, 68, 56, 49, 68,  1, 60, 57, 68])\n"
     ]
    }
   ],
   "source": [
    "# converting the data to a tensor\n",
    "data = torch.tensor(encode(plain_data), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b45a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data in the training and test set\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8bd4f1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([41, 63, 62, 55,  1, 36, 49, 61, 53])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining a block size, sometimes also called the context length\n",
    "# we take block_size+1 because there are 8 examples residing here\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59ac5e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the context is: tensor([41]) the output is: 63\n",
      "When the context is: tensor([41, 63]) the output is: 62\n",
      "When the context is: tensor([41, 63, 62]) the output is: 55\n",
      "When the context is: tensor([41, 63, 62, 55]) the output is: 1\n",
      "When the context is: tensor([41, 63, 62, 55,  1]) the output is: 36\n",
      "When the context is: tensor([41, 63, 62, 55,  1, 36]) the output is: 49\n",
      "When the context is: tensor([41, 63, 62, 55,  1, 36, 49]) the output is: 61\n",
      "When the context is: tensor([41, 63, 62, 55,  1, 36, 49, 61]) the output is: 53\n"
     ]
    }
   ],
   "source": [
    "for i in range(block_size):\n",
    "    print(f\"When the context is: {train_data[:i+1]} the output is: {train_data[i+1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b303b0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs are:\n",
      "torch.Size([1, 8])\n",
      "tensor([[ 1, 69, 64,  1, 68, 56, 53,  1]])\n",
      "************************************************************\n",
      "Outputs are:\n",
      "torch.Size([1, 8])\n",
      "tensor([[69, 64,  1, 68, 56, 53,  1, 64]])\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "batch_size = 1 # number of sequences processed in parellel\n",
    "block_size = 8 # number of tokens (context length) to be considered in each sequence\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Inputs are:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"***\"*20)\n",
    "print(\"Outputs are:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print(\"---\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b5aed26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdef2c30910>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1729)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "086aacd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from the lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) => (batch, time(block), channel (vocab_size))\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # cross entropy needs (B,C,T)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        # idx is (B, T) indeces\n",
    "        for _ in range(max_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time stamp\n",
    "            logits = logits[:,-1,:] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled sequence to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19c48437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 75])\n",
      "tensor(4.9138, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32bae1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ohaylerer Eyoou hang owin he c san o whans\n",
      "Teer ahthoy ned m\n",
      "I acl d f we th myoullig Non\n",
      "Bowaslll e\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long, device = device)\n",
    "print(decode(m.generate(idx, max_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "311740fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a torch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9a3654fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3017683029174805\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    #evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c02ac7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yor it you t g Therevevethe cr\n",
      "Im w buve youxie l ma p p\n",
      "\n",
      "Manou w myou ng fu y oulint the u kitho bl\n",
      "Ca taig I thifr fou otle: t at anhowo fr awald ang ndger mingayoanoe aprereld ckn\n",
      "Thee yo\n",
      "\n",
      "Fiesthe I I ant bry mererthedathe at tof walplst toes I Ancary as you gaclovened d y th dofike p p uss yoon \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ef66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
